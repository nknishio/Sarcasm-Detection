{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL ARCHITECTURE FROM\n",
        "https://arxiv.org/pdf/2406.00367"
      ],
      "metadata": {
        "id": "sf67BBsJX7jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download dataset to local/virtual environment and set up python libraries\n",
        "\n",
        "# !gdown \"https://drive.google.com/uc?id=1YeV-FnAWkPQkpTPgf-ShXhb3SbTb65RP\"\n",
        "!pip install torch transformers pandas numpy scikit-learn nltk"
      ],
      "metadata": {
        "id": "kkC7pEi4NrhR",
        "outputId": "8f3f3120-0fb9-47f1-ae99-34ade3c0493f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in ./myenv/lib/python3.12/site-packages (2.6.0)\r\n",
            "Requirement already satisfied: transformers in ./myenv/lib/python3.12/site-packages (4.51.0)\r\n",
            "Requirement already satisfied: pandas in ./myenv/lib/python3.12/site-packages (2.2.3)\r\n",
            "Requirement already satisfied: numpy in ./myenv/lib/python3.12/site-packages (2.2.4)\r\n",
            "Requirement already satisfied: scikit-learn in ./myenv/lib/python3.12/site-packages (1.6.1)\r\n",
            "Requirement already satisfied: nltk in ./myenv/lib/python3.12/site-packages (3.9.1)\r\n",
            "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from torch) (3.18.0)\r\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.12/site-packages (from torch) (4.13.1)\r\n",
            "Requirement already satisfied: networkx in ./myenv/lib/python3.12/site-packages (from torch) (3.4.2)\r\n",
            "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
            "Requirement already satisfied: fsspec in ./myenv/lib/python3.12/site-packages (from torch) (2025.3.2)\r\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./myenv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./myenv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./myenv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./myenv/lib/python3.12/site-packages (from torch) (9.1.0.70)\r\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./myenv/lib/python3.12/site-packages (from torch) (12.4.5.8)\r\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./myenv/lib/python3.12/site-packages (from torch) (11.2.1.3)\r\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./myenv/lib/python3.12/site-packages (from torch) (10.3.5.147)\r\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./myenv/lib/python3.12/site-packages (from torch) (11.6.1.9)\r\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./myenv/lib/python3.12/site-packages (from torch) (12.3.1.170)\r\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./myenv/lib/python3.12/site-packages (from torch) (0.6.2)\r\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./myenv/lib/python3.12/site-packages (from torch) (2.21.5)\r\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./myenv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./myenv/lib/python3.12/site-packages (from torch) (12.4.127)\r\n",
            "Requirement already satisfied: triton==3.2.0 in ./myenv/lib/python3.12/site-packages (from torch) (3.2.0)\r\n",
            "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch) (78.1.0)\r\n",
            "Requirement already satisfied: sympy==1.13.1 in ./myenv/lib/python3.12/site-packages (from torch) (1.13.1)\r\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./myenv/lib/python3.12/site-packages (from transformers) (0.30.1)\r\n",
            "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from transformers) (24.2)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
            "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from transformers) (2.32.3)\r\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./myenv/lib/python3.12/site-packages (from transformers) (0.21.1)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./myenv/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.12/site-packages (from transformers) (4.67.1)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in ./myenv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGqJIRVMANGX"
      },
      "source": [
        "# Mount Google Drive and load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "uhqEnHFvALHL",
        "outputId": "0674521c-3316-4001-b8df-aa1945ce72e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 28619\n",
            "is_sarcastic\n",
            "0    14985\n",
            "1    13634\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/user96/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/user96/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   is_sarcastic                                           headline\n",
              "0             1  thirtysomethe scientist unveil doomsday clock ...\n",
              "1             0  dem rep totally nail why congress be fall shor...\n",
              "2             0     eat your veggie 9 deliciously different recipe\n",
              "3             1         inclement weather prevent liar get to work\n",
              "4             1  mother come pretty close to use word streaming..."
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>thirtysomethe scientist unveil doomsday clock ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>dem rep totally nail why congress be fall shor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>eat your veggie 9 deliciously different recipe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>inclement weather prevent liar get to work</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>mother come pretty close to use word streaming...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#for nonlocal runtime\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/Kaggle Club/SARCASM PROJECT '25/cleaned_reddit_comments.csv\").fillna(' ')\n",
        "\n",
        "# Load the CSV dataset (adjust the file path as needed)\n",
        "df = pd.read_csv('cleaned_NewsHeadlines_comments.csv', usecols=['is_sarcastic', 'headline']).fillna('')\n",
        "\n",
        "print(\"Dataset size:\", len(df))\n",
        "print(df['is_sarcastic'].value_counts())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     # Lowercase the text\n",
        "#     text = text.lower()\n",
        "#     # Remove URLs\n",
        "#     text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "#     # Remove non-alphanumeric characters; keep letters and spaces\n",
        "#     text = re.sub(r'[^a-z\\s]', '', text)\n",
        "#     # Tokenize text\n",
        "#     tokens = text.split()\n",
        "#     # Remove stopwords\n",
        "#     tokens = [word for word in tokens if word not in stop_words]\n",
        "#     # Lemmatize tokens\n",
        "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "#     return ' '.join(tokens)\n",
        "\n",
        "# # Apply preprocessing to the comment column\n",
        "# df['comment'] = df['comment'].apply(preprocess_text)\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "EBqjQ5Gk7osL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, split into (train+validation) vs. test (90% vs. 10%)\n",
        "train_val_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.10,\n",
        "    random_state=42,\n",
        "    stratify=df['is_sarcastic']\n",
        ")\n",
        "\n",
        "# Now split the train_val_df into training and validation sets (90% of 90% becomes 81%, and 10% of 90% becomes 9%)\n",
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.10,\n",
        "    random_state=42,\n",
        "    stratify=train_val_df['is_sarcastic']\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Validation size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))"
      ],
      "metadata": {
        "id": "CM-TjKVcYmqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de57dfe-af63-4e73-9840-e98716532dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 23181\n",
            "Validation size: 2576\n",
            "Test size: 2862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture\n",
        "\n",
        "\n",
        "1. RoBERTa Encoder:\n",
        "* Load a pretrained RoBERTa (roberta-base) model and its tokenizer from the HuggingFace library.\n",
        "* Use RoBERTa to obtain the embedding matrix for the input text.\n",
        "2. Dropout Layer:\n",
        "* Applied to the embeddings to prevent overfitting. In the paper, a dropout rate of 0.1 is used.\n",
        "3. BiLSTM Layer:\n",
        "* Processes the embeddings bidirectionally to capture long-range dependencies.\n",
        "* The paper experiments with different hidden unit sizes (e.g., 128, 256, 512). For the BiLSTM, the effective hidden size is doubled due to its forward and backward processing.\n",
        "4. Flatten and Dense Layers:\n",
        "* The output of the BiLSTM is flattened.\n",
        "* One or two fully connected layers are used to capture the relationship between the features and the final sentiment classes.\n",
        "5. Classification (Softmax) Layer:\n",
        "* A Softmax function is applied to output the probability distribution over sentiment classes (e.g., positive, negative, neutral)."
      ],
      "metadata": {
        "id": "L32xLiLRW_Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "class RoBERTa_BiLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size=256, num_classes=2, dropout_rate=0.1):\n",
        "        super(RoBERTa_BiLSTM, self).__init__()\n",
        "        # Load pretrained RoBERTa model and tokenizer\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "        # BiLSTM: input size from RoBERTa’s hidden size (768)\n",
        "        self.bilstm = nn.LSTM(input_size=768, hidden_size=hidden_size,\n",
        "                              num_layers=1, batch_first=True,\n",
        "                              bidirectional=True)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        # After BiLSTM, output feature size becomes hidden_size * 2\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Obtain embeddings from RoBERTa (last hidden state)\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state  # shape: (batch_size, seq_len, 768)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(sequence_output)\n",
        "        # Process with BiLSTM layer\n",
        "        lstm_out, _ = self.bilstm(x)  # shape: (batch_size, seq_len, hidden_size*2)\n",
        "\n",
        "        # Use mean pooling over the sequence length to aggregate features\n",
        "        pooled_output = torch.mean(lstm_out, dim=1)  # shape: (batch_size, hidden_size*2)\n",
        "\n",
        "        # Final classification layer\n",
        "        logits = self.fc(pooled_output)\n",
        "        probabilities = self.softmax(logits)\n",
        "        return logits, probabilities\n",
        "\n",
        "    def tokenize_texts(self, texts, max_length=128):\n",
        "        # Tokenize and encode the texts using the RoBERTa tokenizer\n",
        "        encoding = self.tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "        return encoding['input_ids'], encoding['attention_mask']\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    model = RoBERTa_BiLSTM(hidden_size=256, num_classes=2, dropout_rate=0.1)\n",
        "    texts = [\"this movie was great\", \"this was a bad experience\"]\n",
        "    input_ids, attention_mask = model.tokenize_texts(texts)\n",
        "\n",
        "    logits, probabilities = model(input_ids, attention_mask)\n",
        "    print(\"Logits:\", logits)\n",
        "    print(\"Probabilities:\", probabilities)"
      ],
      "metadata": {
        "id": "f-1YW_JZW_Fy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89159e18-2c6c-444d-f0e7-d996a26fb789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.0531,  0.0143],\n",
            "        [-0.0439,  0.0206]], grad_fn=<AddmmBackward0>)\n",
            "Probabilities: tensor([[0.4832, 0.5168],\n",
            "        [0.4839, 0.5161]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek4Hc9tdAPpf"
      },
      "source": [
        "# Training + Hyperparameters\n",
        "\n",
        "* **Learning Rates (l)**: Experiment with l ∈ {0.0001, 0.00001, 0.000001}. In the best-case experimental settings (as per the paper), use 0.00001.\n",
        "* **Hidden Units (h)**: For the RNN layer, experiment with 128, 256, or 512 units. Note that for BiLSTM, the effective output dimension is 2×h.\n",
        "* **Dropout Rate**: 0.1\n",
        "* **Epochs**: 5 (as stated in the paper)\n",
        "* **Optimizer**: AdamW is recommended."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NewsHeadlinesDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        # Store the preprocessed texts and labels\n",
        "        self.labels = df['is_sarcastic'].values\n",
        "        self.comments = df['headline'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the comment and label for a given index\n",
        "        comment = str(self.comments[idx])\n",
        "        label = self.labels[idx]\n",
        "        return comment, label\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = NewsHeadlinesDataset(train_df)\n",
        "val_dataset = NewsHeadlinesDataset(val_df)\n",
        "test_dataset = NewsHeadlinesDataset(test_df)\n",
        "\n",
        "# Create DataLoaders for batching (adjust batch_size as needed)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "dultTdWJYvOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)  # or try lr in {1e-4, 1e-6}\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "epochs = 20\n",
        "patience = 5 # Number of epochs to wait for improvement before stopping\n",
        "best_val_loss = float('inf')\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for comments, labels in train_loader:\n",
        "        # Tokenize batch of comments\n",
        "        input_ids, attention_mask = model.tokenize_texts(list(comments))\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Evaluate on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for comments, labels in val_loader:\n",
        "            input_ids, attention_mask = model.tokenize_texts(list(comments))\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = torch.tensor(labels).to(device)\n",
        "\n",
        "            logits, _ = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Check for early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(f\"Early stopping activated. No improvement in validation loss for {patience} consecutive epochs.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "UZtmUExtXw8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776759a7-930c-406f-98a4-846b8655cedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_14497/2856251614.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n",
            "/tmp/ipykernel_14497/2856251614.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 0.0879 - Val Loss: 0.4122\n",
            "Epoch 2/20 - Train Loss: 0.0793 - Val Loss: 0.3409\n",
            "Epoch 3/20 - Train Loss: 0.0647 - Val Loss: 0.3529\n",
            "Epoch 4/20 - Train Loss: 0.0565 - Val Loss: 0.3649\n",
            "Epoch 5/20 - Train Loss: 0.0523 - Val Loss: 0.4347\n",
            "Epoch 6/20 - Train Loss: 0.0467 - Val Loss: 0.3663\n",
            "Epoch 7/20 - Train Loss: 0.0407 - Val Loss: 0.3330\n",
            "Epoch 8/20 - Train Loss: 0.0354 - Val Loss: 0.3559\n",
            "Epoch 9/20 - Train Loss: 0.0351 - Val Loss: 0.4151\n",
            "Epoch 10/20 - Train Loss: 0.0344 - Val Loss: 0.4362\n",
            "Epoch 11/20 - Train Loss: 0.0330 - Val Loss: 0.4406\n",
            "Epoch 12/20 - Train Loss: 0.0304 - Val Loss: 0.4021\n",
            "Early stopping activated. No improvement in validation loss for 5 consecutive epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Metrics"
      ],
      "metadata": {
        "id": "-UkklgdFX0df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for comments, labels in test_loader:\n",
        "        input_ids, attention_mask = model.tokenize_texts(list(comments))\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = torch.tensor(labels).to(device)\n",
        "\n",
        "        logits, _ = model(input_ids, attention_mask)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"Weighted Precision:\", precision)\n",
        "print(\"Weighted Recall:\", recall)\n",
        "print(\"Weighted F1-Score:\", f1)"
      ],
      "metadata": {
        "id": "LXEmjzPIX2Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e060ed8f-9420-4a8d-ee81-fdbea37d55dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_14497/1447073919.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8983228511530398\n",
            "Weighted Precision: 0.898512501241357\n",
            "Weighted Recall: 0.8983228511530398\n",
            "Weighted F1-Score: 0.898232422618565\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}